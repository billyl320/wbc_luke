
R version 3.4.4 (2018-03-15) -- "Someone to Lean On"
Copyright (C) 2018 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> rm(list=ls())
> 
> 
> library(xtable) #for table creation for latex
> library(e1071)#for svm
> 
> #reporting session info
> sessionInfo()
R version 3.4.4 (2018-03-15)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 18.04.5 LTS

Matrix products: default
BLAS: /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] e1071_1.7-3  xtable_1.8-4

loaded via a namespace (and not attached):
[1] compiler_3.4.4 class_7.3-17  
> 
> 
> #shape metrics
> da_a = read.table('all_SHAPES.txt', sep=',', header=TRUE)
> da_h = read.table('hem_SHAPES.txt', sep=',', header=TRUE)
> 
> data = rbind(da_a, da_h)
> 
> #eis
> ei_a = read.table('all.txt', sep=',', header=TRUE)
> ei_h = read.table('hem.txt', sep=',', header=TRUE)
> 
> ei = rbind(ei_a, ei_h)
> #obtaining sp values
> sp<-ei[,1]/(ei[,1]+ei[,2])
> 
> #color
> color_a = read.table('all_COLORS.txt', sep=',', header=TRUE)
> color_h = read.table('hem_COLORS.txt', sep=',', header=TRUE)
> 
> color = rbind(color_a, color_h)
> 
> #texture
> texture_a = read.table('all_TEXTURE.txt', sep=',', header=TRUE)
> texture_h = read.table('hem_TEXTURE.txt', sep=',', header=TRUE)
> 
> texture = rbind(texture_a, texture_h)
> 
> labs2<-as.factor(c(
+                   rep("all", dim(ei_a)[1]),
+                   rep("hem", dim(ei_h)[1])    ) )
> 
> 
> labs<-as.factor(c(
+                   rep(1, dim(ei_a)[1]),
+                   rep(2, dim(ei_h)[1])    ) )
> 
> #counts plot
> temp<-as.data.frame(cbind(ei,sp, color, texture, data))
> 
> #setup for SVM model
> 
> train<-as.data.frame(cbind(as.factor(labs), temp))
> colnames(train)[1]<-"labs_svm"
> 
> #variables to keep
> keep<-c(1:25)
> 
> #now let's tune the svm model using 5-folds on t-set and validaiton
> 
> set.seed(2405)
> 
> keep2<-which(train$labs_svm==1)
> keep3<-which(train$labs_svm==2)
> 
> #80% for training and 20% for validation
> obs_1 = sample(keep2, floor(length(keep2)*0.80) )
> obs_2 = sample(keep3, floor(length(keep3)*0.80) )
> 
> obs<-c(obs_1, obs_2)
> 
> tc <- tune.control(cross = 5)
> 
> tune.out<-tune(svm, as.factor(labs_svm) ~.,
+           data=train[obs, keep],
+           kernel='radial',
+           ranges=list(cost=c(1:2, 5, 10),
+                       gamma=c(1/dim(train)[1], 1/dim(train[,keep])[2], 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.75, 1, 1.5 )
+                       ) ,
+           tunecontrol = tc)
> 
> summary(tune.out)

Parameter tuning of ‘svm’:

- sampling method: 5-fold cross validation 

- best parameters:
 cost gamma
    1  0.05

- best performance: 0.02891986 

- Detailed performance results:
   cost       gamma      error  dispersion
1     1 0.003846154 0.19303136 0.128693128
2     2 0.003846154 0.13530778 0.098792419
3     5 0.003846154 0.12090592 0.090096247
4    10 0.003846154 0.10150987 0.072321688
5     1 0.040000000 0.04343786 0.026619540
6     2 0.040000000 0.03379791 0.021691275
7     5 0.040000000 0.03855981 0.013473457
8    10 0.040000000 0.03855981 0.013473457
9     1 0.050000000 0.02891986 0.020168635
10    2 0.050000000 0.02891986 0.020168635
11    5 0.050000000 0.03368177 0.013261542
12   10 0.050000000 0.03855981 0.013473457
13    1 0.100000000 0.05772358 0.013328760
14    2 0.100000000 0.05284553 0.010404456
15    5 0.100000000 0.05772358 0.013328760
16   10 0.100000000 0.05296167 0.020281187
17    1 0.150000000 0.06236934 0.020912375
18    2 0.150000000 0.04808362 0.000636147
19    5 0.150000000 0.05296167 0.011308396
20   10 0.150000000 0.05296167 0.011308396
21    1 0.200000000 0.06736353 0.035445339
22    2 0.200000000 0.06724739 0.030943173
23    5 0.200000000 0.06724739 0.030943173
24   10 0.200000000 0.06724739 0.030943173
25    1 0.250000000 0.06736353 0.035445339
26    2 0.250000000 0.07212544 0.029176200
27    5 0.250000000 0.07212544 0.029176200
28   10 0.250000000 0.07212544 0.029176200
29    1 0.300000000 0.07711963 0.039704865
30    2 0.300000000 0.07212544 0.029176200
31    5 0.300000000 0.07212544 0.029176200
32   10 0.300000000 0.07212544 0.029176200
33    1 0.350000000 0.09628339 0.038129924
34    2 0.350000000 0.08188153 0.036587670
35    5 0.350000000 0.08188153 0.036587670
36   10 0.350000000 0.08188153 0.036587670
37    1 0.400000000 0.12520325 0.052291579
38    2 0.400000000 0.10116144 0.043426837
39    5 0.400000000 0.10116144 0.043426837
40   10 0.400000000 0.10116144 0.043426837
41    1 0.450000000 0.12996516 0.060206700
42    2 0.450000000 0.12520325 0.066596468
43    5 0.450000000 0.12520325 0.066596468
44   10 0.450000000 0.12520325 0.066596468
45    1 0.500000000 0.13960511 0.070729466
46    2 0.500000000 0.12996516 0.060206700
47    5 0.500000000 0.12996516 0.060206700
48   10 0.500000000 0.12996516 0.060206700
49    1 0.750000000 0.21173055 0.095335069
50    2 0.750000000 0.18281069 0.085562558
51    5 0.750000000 0.18281069 0.085562558
52   10 0.750000000 0.18281069 0.085562558
53    1 1.000000000 0.22624855 0.102842069
54    2 1.000000000 0.21672474 0.103672281
55    5 1.000000000 0.21672474 0.103672281
56   10 1.000000000 0.21672474 0.103672281
57    1 1.500000000 0.37909408 0.182519595
58    2 1.500000000 0.35493612 0.178681354
59    5 1.500000000 0.35493612 0.178681354
60   10 1.500000000 0.35493612 0.178681354

> 
> #training data
> ypred=predict(tune.out$best.model ,train[obs,])
> table(predict=ypred, truth=train$labs_svm[obs])
       truth
predict   1   2
      1 102   1
      2   2 103
> mean(ypred==as.factor(as.numeric(train$labs_svm[obs])))
[1] 0.9855769
> 
> #validation data
> ypred=predict(tune.out$best.model ,train[-obs,])
> table(predict=ypred, truth=train$labs_svm[-obs])
       truth
predict  1  2
      1 24  0
      2  2 26
> tab_v<-table(predict=ypred, truth=train$labs_svm[-obs])
> mean(ypred==as.factor(as.numeric(train$labs_svm[-obs])))
[1] 0.9615385
> 
> #obtaining 95% CI
> binom.test(x=sum(diag(tab_v)), n=sum(tab_v), p= mean(ypred==as.factor(as.numeric(train$labs_svm[-obs]))) )

	Exact binomial test

data:  sum(diag(tab_v)) and sum(tab_v)
number of successes = 50, number of trials = 52, p-value = 1
alternative hypothesis: true probability of success is not equal to 0.9615385
95 percent confidence interval:
 0.8678716 0.9953077
sample estimates:
probability of success 
             0.9615385 

> 
> #calculating varibale importance
> w <- t(tune.out$best.model$coefs) %*% tune.out$best.model$SV    # weight vectors
> w <- apply(w, 2, function(v){sqrt(sum(v^2))})                   # weight
> w_sort <- sort(w, decreasing = T)
> 
> #max weight
> w_max<-(w_sort[1])
> 
> #normalized weights relative to the max
> w_norm<- w_sort / w_max
> 
> #table for Latex
> #normalized
> xtable(as.matrix(w_norm), digits=3)
% latex table generated in R 3.4.4 by xtable 1.8-4 package
% Wed Sep 30 12:38:43 2020
\begin{table}[ht]
\centering
\begin{tabular}{rr}
  \hline
 & x \\ 
  \hline
Mean\_B & 1.000 \\ 
  Mean\_K & 0.998 \\ 
  Shape\_circ & 0.886 \\ 
  SD\_R & 0.823 \\ 
  Mean\_M & 0.780 \\ 
  SD\_Y & 0.750 \\ 
  Mean\_Y & 0.739 \\ 
  Shape\_e1 & 0.710 \\ 
  sp & 0.678 \\ 
  Shape\_e2 & 0.666 \\ 
  black & 0.534 \\ 
  Mean\_C & 0.518 \\ 
  SD\_G & 0.467 \\ 
  white & 0.443 \\ 
  SD\_B & 0.432 \\ 
  SD\_K & 0.425 \\ 
  Mean\_P & 0.391 \\ 
  SD\_P & 0.390 \\ 
  Mean\_R & 0.338 \\ 
  Shape\_eccent & 0.296 \\ 
  SD\_M & 0.258 \\ 
  Shape\_corn & 0.156 \\ 
  SD\_C & 0.130 \\ 
  Mean\_G & 0.035 \\ 
   \hline
\end{tabular}
\end{table}
> 
> #table for Latex
> #non normalized
> xtable(as.matrix(w), digits=3)
% latex table generated in R 3.4.4 by xtable 1.8-4 package
% Wed Sep 30 12:38:43 2020
\begin{table}[ht]
\centering
\begin{tabular}{rr}
  \hline
 & x \\ 
  \hline
white & 5.621 \\ 
  black & 6.775 \\ 
  sp & 8.605 \\ 
  Mean\_R & 4.287 \\ 
  SD\_R & 10.450 \\ 
  Mean\_G & 0.442 \\ 
  SD\_G & 5.931 \\ 
  Mean\_B & 12.698 \\ 
  SD\_B & 5.488 \\ 
  Mean\_C & 6.582 \\ 
  SD\_C & 1.655 \\ 
  Mean\_M & 9.904 \\ 
  SD\_M & 3.282 \\ 
  Mean\_Y & 9.384 \\ 
  SD\_Y & 9.521 \\ 
  Mean\_K & 12.677 \\ 
  SD\_K & 5.392 \\ 
  Mean\_P & 4.967 \\ 
  SD\_P & 4.958 \\ 
  Shape\_circ & 11.246 \\ 
  Shape\_eccent & 3.761 \\ 
  Shape\_e1 & 9.013 \\ 
  Shape\_e2 & 8.459 \\ 
  Shape\_corn & 1.987 \\ 
   \hline
\end{tabular}
\end{table}
> 
> #VarImp based on feature type
> vars_c<-c(4:17)
> vars_t<-c(18:19)
> 
> #vImp<-matrix(nrow=1, ncol=3, data=0)
> vImp<-c(0,0,0)
> names(vImp)<-c("Color", "Texture", "Shape")
> 
> vImp[1]<-sum(w[vars_c])
> vImp[2]<-sum(w[vars_t])
> vImp[3]<-sum(w[-c(vars_t, vars_c)])
> 
> vImp_sort <- sort(vImp, decreasing = T)
> 
> xtable(t(as.matrix(vImp_sort/vImp_sort[1])), digits=3 )
% latex table generated in R 3.4.4 by xtable 1.8-4 package
% Wed Sep 30 12:38:43 2020
\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \hline
 & Color & Shape & Texture \\ 
  \hline
1 & 1.000 & 0.568 & 0.102 \\ 
   \hline
\end{tabular}
\end{table}
> 
> #
> 
> proc.time()
   user  system elapsed 
  5.755   0.325   5.520 
